GNU 13.2.0 is now loaded
Loading module for CUDA 11.8
CUDA 11.8 is now loaded
Loading module for Machine Learning 2024.01
Machine Learning 2024.01 is now loaded

Loading machine_learning/2024.01
  Loading requirement: gcc/13.2.0 cuda/11.8
Loading module for pytorch 2.0
pytorch 2.0 is now loaded
Dataset mean: 0.1799, std: 0.7484
Epoch [1/100] - SupCon Loss: 2.1685
Epoch [2/100] - SupCon Loss: 1.8014
Epoch [3/100] - SupCon Loss: 1.6978
Epoch [4/100] - SupCon Loss: 1.6348
Epoch [5/100] - SupCon Loss: 1.5955
Epoch [6/100] - SupCon Loss: 1.5660
Epoch [7/100] - SupCon Loss: 1.5303
Epoch [8/100] - SupCon Loss: 1.5067
Epoch [9/100] - SupCon Loss: 1.4873
Epoch [10/100] - SupCon Loss: 1.4710
Epoch [11/100] - SupCon Loss: 1.4497
Epoch [12/100] - SupCon Loss: 1.4403
Epoch [13/100] - SupCon Loss: 1.4328
Epoch [14/100] - SupCon Loss: 1.4221
Epoch [15/100] - SupCon Loss: 1.4167
Epoch [16/100] - SupCon Loss: 1.4153
Epoch [17/100] - SupCon Loss: 1.3963
Epoch [18/100] - SupCon Loss: 1.3873
Epoch [19/100] - SupCon Loss: 1.3723
Epoch [20/100] - SupCon Loss: 1.3689
Epoch [21/100] - SupCon Loss: 1.3583
Epoch [22/100] - SupCon Loss: 1.3572
Epoch [23/100] - SupCon Loss: 1.3556
Epoch [24/100] - SupCon Loss: 1.3455
Epoch [25/100] - SupCon Loss: 1.3424
Epoch [26/100] - SupCon Loss: 1.3326
Epoch [27/100] - SupCon Loss: 1.3211
Epoch [28/100] - SupCon Loss: 1.3225
Epoch [29/100] - SupCon Loss: 1.3212
Epoch [30/100] - SupCon Loss: 1.3158
Epoch [31/100] - SupCon Loss: 1.3074
Epoch [32/100] - SupCon Loss: 1.3070
Epoch [33/100] - SupCon Loss: 1.2966
Epoch [34/100] - SupCon Loss: 1.2882
Epoch [35/100] - SupCon Loss: 1.2866
Epoch [36/100] - SupCon Loss: 1.2752
Epoch [37/100] - SupCon Loss: 1.2687
Epoch [38/100] - SupCon Loss: 1.2721
Epoch [39/100] - SupCon Loss: 1.2667
Epoch [40/100] - SupCon Loss: 1.2624
Epoch [41/100] - SupCon Loss: 1.2531
Epoch [42/100] - SupCon Loss: 1.2540
Epoch [43/100] - SupCon Loss: 1.2436
Epoch [44/100] - SupCon Loss: 1.2376
Epoch [45/100] - SupCon Loss: 1.2351
Epoch [46/100] - SupCon Loss: 1.2356
Epoch [47/100] - SupCon Loss: 1.2240
Epoch [48/100] - SupCon Loss: 1.2261
Epoch [49/100] - SupCon Loss: 1.2270
Epoch [50/100] - SupCon Loss: 1.2179
Epoch [51/100] - SupCon Loss: 1.2189
Epoch [52/100] - SupCon Loss: 1.2037
Epoch [53/100] - SupCon Loss: 1.2054
Epoch [54/100] - SupCon Loss: 1.1969
Epoch [55/100] - SupCon Loss: 1.1930
Epoch [56/100] - SupCon Loss: 1.1940
Epoch [57/100] - SupCon Loss: 1.1817
Epoch [58/100] - SupCon Loss: 1.1774
Epoch [59/100] - SupCon Loss: 1.1780
Epoch [60/100] - SupCon Loss: 1.1670
Epoch [61/100] - SupCon Loss: 1.1595
Epoch [62/100] - SupCon Loss: 1.1653
Epoch [63/100] - SupCon Loss: 1.1556
Epoch [64/100] - SupCon Loss: 1.1551
Epoch [65/100] - SupCon Loss: 1.1524
Epoch [66/100] - SupCon Loss: 1.1527
Epoch [67/100] - SupCon Loss: 1.1444
Epoch [68/100] - SupCon Loss: 1.1326
Epoch [69/100] - SupCon Loss: 1.1380
Epoch [70/100] - SupCon Loss: 1.1269
Epoch [71/100] - SupCon Loss: 1.1315
Epoch [72/100] - SupCon Loss: 1.1258
Epoch [73/100] - SupCon Loss: 1.1222
Epoch [74/100] - SupCon Loss: 1.1098
Epoch [75/100] - SupCon Loss: 1.1150
Epoch [76/100] - SupCon Loss: 1.1176
Epoch [77/100] - SupCon Loss: 1.1083
Epoch [78/100] - SupCon Loss: 1.0984
Epoch [79/100] - SupCon Loss: 1.0882
Epoch [80/100] - SupCon Loss: 1.0992
Epoch [81/100] - SupCon Loss: 1.0926
Epoch [82/100] - SupCon Loss: 1.0817
Epoch [83/100] - SupCon Loss: 1.0820
Epoch [84/100] - SupCon Loss: 1.0762
Epoch [85/100] - SupCon Loss: 1.0785
Epoch [86/100] - SupCon Loss: 1.0738
Epoch [87/100] - SupCon Loss: 1.0676
Epoch [88/100] - SupCon Loss: 1.0697
Epoch [89/100] - SupCon Loss: 1.0681
Epoch [90/100] - SupCon Loss: 1.0674
Epoch [91/100] - SupCon Loss: 1.0569
Epoch [92/100] - SupCon Loss: 1.0635
Epoch [93/100] - SupCon Loss: 1.0569
Epoch [94/100] - SupCon Loss: 1.0514
Epoch [95/100] - SupCon Loss: 1.0553
Epoch [96/100] - SupCon Loss: 1.0536
Epoch [97/100] - SupCon Loss: 1.0404
Epoch [98/100] - SupCon Loss: 1.0443
Epoch [99/100] - SupCon Loss: 1.0421
Epoch [100/100] - SupCon Loss: 1.0385
Class counts: [1276, 1357, 2157, 975, 991, 1773, 1521, 1513, 2038, 1149, 1083, 1912, 1365, 1623, 1104, 1152, 1505, 1490, 1632, 1882, 609, 913, 2518, 1807, 2209, 2121, 1331, 1171, 1593, 1850, 673, 1600, 896, 1551, 2275, 1705, 1240]
Epoch 1/20, Loss: 0.4523
F1 Score: 0.7353
Epoch 2/20, Loss: 0.2563
F1 Score: 0.7416
Epoch 3/20, Loss: 0.2151
F1 Score: 0.7429
Epoch 4/20, Loss: 0.1992
F1 Score: 0.7385
Epoch 5/20, Loss: 0.1837
F1 Score: 0.7451
Epoch 6/20, Loss: 0.1746
F1 Score: 0.7431
Epoch 7/20, Loss: 0.1655
F1 Score: 0.7445
Epoch 8/20, Loss: 0.1581
F1 Score: 0.7436
Epoch 9/20, Loss: 0.1532
F1 Score: 0.7449
Epoch 10/20, Loss: 0.1479
F1 Score: 0.7437
Epoch 11/20, Loss: 0.1421
F1 Score: 0.7428
Epoch 12/20, Loss: 0.1395
F1 Score: 0.7419
Epoch 13/20, Loss: 0.1357
F1 Score: 0.7421
Epoch 14/20, Loss: 0.1326
F1 Score: 0.7448
Epoch 15/20, Loss: 0.1288
F1 Score: 0.7412
Epoch 16/20, Loss: 0.1259
F1 Score: 0.7419
Epoch 17/20, Loss: 0.1238
F1 Score: 0.7432
Epoch 18/20, Loss: 0.1209
F1 Score: 0.7441
Epoch 19/20, Loss: 0.1185
F1 Score: 0.7439
Epoch 20/20, Loss: 0.1167
F1 Score: 0.7410
Validation Performance:
F1 Score: 0.7410
Test Performance:
F1 Score: 0.7421
/sw/rl9g/pytorch-2.0/conda3env/env/lib/python3.8/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
